# input.yaml for fine-tuning SevenNet-0

model:
    chemical_species: 'Auto'
    cutoff: 5.0
    channel: 128
    lmax: 2
    num_convolution_layer: 5

    irreps_manual:
        - "128x0e"
        - "128x0e+64x1e+32x2e"
        - "128x0e+64x1e+32x2e"
        - "128x0e+64x1e+32x2e"
        - "128x0e+64x1e+32x2e"
        - "128x0e"

    weight_nn_hidden_neurons: [64, 64]
    radial_basis:
        radial_basis_name: 'bessel'
        bessel_basis_num: 8
    cutoff_function:
        cutoff_function_name: 'XPLOR'
        cutoff_on: 4.5

    act_gate: {'e': 'silu', 'o': 'tanh'}
    act_scalar: {'e': 'silu', 'o': 'tanh'}

    is_parity: False
    optimize_by_reduce: True
    self_connection_type: 'linear'

    # ~~~~~ Above settings (which defines model) should be maintained for fine tuning ~~~~~~ #

    #avg_num_neigh: 1368.726 # Denominator for convolution layers in SevenNet 0 (root(1368) = 39), will be read from checkpoint.
    train_shift_scale: True
    train_avg_num_neigh: True

train:
    random_seed: 1
    is_train_stress: True
    epoch: 100

    optimizer: 'adam'
    optim_param:
        lr: 0.004
    scheduler: 'exponentiallr'
    scheduler_param:
        gamma: 0.99

    force_loss_weight   : 0.1
    stress_loss_weight  : 1e-06

    per_epoch: 10
    # TotalEnergy, Energy, Force, Stress, Stress_GPa, TotalLoss
    # RMSE, MAE, Loss available
    error_record:
        - ['Energy', 'RMSE']
        - ['Force', 'RMSE']
        - ['Stress', 'RMSE']
        - ['TotalLoss', 'None']

    continue:
        reset_optimizer: True
        reset_scheduler: True
        reset_epoch: True
        checkpoint: './checkpoint_sevennet_0.pth'
        use_statistic_values_of_checkpoint: True # Set True to use shift, scale, and avg_num_neigh from checkpoint or not

    # If the dataset changed (for fine-tuning),
    # setting 'use_statistic_value_of_checkpoint' to False roughly changes model's accuracy in the beginning of training.
    # We recommand to use it as 'False', and turn train_shift_scale and train_avg_num_neigh to 'True' for fine-tuning dataset adaptation.

data:
    batch_size: 8
    data_divide_ratio: 0.1

    use_species_wise_shift_scale: True  # should be True for fine-tuning SevenNet-0

    #data_format: 'ase'                           # Default is 'structure_list'
    #data_format_args:                            # Paramaters, will be passed to ase.io.read
        #index: '-10:'

    # ASE tries to infer its type by extension, in this case, extxyz file is loaded by ase.
    #load_dataset_path: ['../data/test.extxyz']   # Example of using ase as data_format

    # If only load_dataset_path is provided, train/valid set is automatically decided by splitting dataset by divide ratio
    # If both load_dataset_path & load_validset_path is provided, use load_dataset_path as training set.
    load_dataset_path: ['structure_list']
    #load_validset_path: ['./valid.sevenn_data']

    #save_dataset_path: 'total'
    #save_by_train_valid: True
    #save_by_label: False
