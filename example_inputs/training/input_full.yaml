# Example input.yaml for training SEVENNet.
# The underlying model is identical to nequip (https://github.com/mir-group/nequip), but the names of hyperparameters might different.
# Except channel, and num_convolution_layer, which has minimal values to quickly check the installation, values written here normally works well.
# Defaults that works well of channel, lmax and num_convolution_layer are 32, 1, 3 respectively.

model:
    chemical_species: 'Auto'                      # Chemical symbols present in the dataset, guess from load_dataset data if 'auto'
    cutoff: 4.0                                   # Cutoff radius in Angstroms. If two atoms are within the cutoff, they are connected.
    channel: 4                                    # Equivalent to 'num_features' in nequip. Represents the multiplicity of node features. 32 is recommended as default.
    lmax: 1                                       # Maximum order of irreps (rotation order). 1 is recommended as default
    num_convolution_layer: 4                      # Equivalent to 'num_layers' in nequip. Represents the number of message passing layers in the model. 3 is recommended as default

    weight_nn_hidden_neurons: [64, 64]            # Equivalent to 'invariant_layers' and 'neurons' in nequip. Represents the neural network for the radial basis
    radial_basis:                                 # Function and its parameters to encode radial distance
        radial_basis_name: 'bessel'               # Only 'bessel' is currently supported
        bessel_basis_num: 8                       # Equivalent to 'num_basis' in nequip. Represents the number of Bessel functions as the radial basis
    cutoff_function:                              # Differentiable, decaying function to encode radial distance
        cutoff_function_name: 'poly_cut'          # Only 'poly_cut' is currently supported
        poly_cut_p_value: 6                       # Equivalent to 'PolynomialCutoff_p' in nequip. Smaller p values correspond to a stronger decay with distance 

    # For even nonlinearities, 'silu', 'abs' (absolute), and 'ssp' (shifted softmax) are supported. Defaults generally work well
    act_gate: {'e': 'silu', 'o': 'tanh'}          # Equivalent to 'nonlinearity_gates' in nequip.
    act_scalar: {'e': 'silu', 'o': 'tanh'}        # Equivalent to 'nonlinearity_scalars' in nequip.

    is_parity: True                               # Pairy False (to SE(3) group) is not supported yet (on working!)

    train_shift_scale   : False                   # Enable training for shift & scale. Useful if the dataset is augmented
    train_avg_num_neigh : False                   # Enable training for avg_num_neigh. Useful if the dataset is augmented
    optimize_by_reduce: False                     # Speed-up by eliminating non-scalar computations at the last layer
                                                                  
train:
    random_seed: 1                                # Random seed for pytorch
    is_train_stress     : False                   # Includes stress in the loss function
    epoch: 10                                     # Ends training after this number of epochs

    # Each optimizer and scheduler have different available parameters. 
    # You can refer to sevenn/train/optim.py for supporting optimizer & schedulers
    optimizer: 'adam'                             # Options available are 'sgd', 'adagrad', 'adam', 'adamw', 'radam'
    optim_param:                                  
        lr: 0.005
    scheduler: 'exponentiallr'                    # Options available are 'steplr', 'multisteplr', 'exponentiallr', 'cosineannealinglr'
    scheduler_param:
        gamma: 0.99

    force_loss_weight   : 0.1                     # Coefficient for force loss
    stress_loss_weight  : 1e-06                   # Coefficient for stress loss (It is applied to kbar unit)

    per_epoch:  10                                 # Generate checkpoints every this epoch

    # TotalEnergy, Energy, Force, Stress, Stress_GPa, TotalLoss
    # RMSE, MAE, Loss available
    error_record:
        - ['Energy', 'RMSE']
        - ['Force', 'RMSE']
        - ['Stress', 'RMSE']
        - ['TotalLoss', 'None']

    #continue: 
        #reset_optimizer: False 
        #reset_scheduler: False  
        #checkpoint: 'path_to_previous_checkpoint/checkpoint_best.pth'
        #use_statistic_values_of_checkpoint: False # whether use shift, scale, and avg_num_neigh from checkpoint or update from new dataset

data:
    batch_size: 2                                 # Batch size. If training fails due to memory shortage, lower this value
    data_divide_ratio: 0.1                        # Divide the whole dataset into training and validation sets by this ratio

    #shift: 0.0                                   # User defined global shift
    #scale: 1.0                                   # User defined global scale

    # Use element wise shift, scale (also trainable).
    # The starting values are linear fitted element reference atomic energy and element wise force rms
    use_species_wise_shift_scale: False           

    # ase.io.read readable data files or structure_list or .sevenn_data files can be used as dataset.
    # .sevenn_data is preprocessed data set has edges connected (can be obtained by using sevenn_graph_build or by save_** options below)
    #data_format: 'ase'                           # Default is 'structure_list'
    #data_format_args:                            # Paramaters, will be passed to ase.io.read
        #index: '-10:'

    # ASE tries to infer its type by extension, in this case, extxyz file is loaded by ase.
    #load_dataset_path: ['../data/test.extxyz']   # Example of using ase as data_format

    # If only load_dataset_path is provided, train/valid set is automatically decided by splitting dataset by divide ratio
    # If both load_dataset_path & load_validset_path is provided, use load_dataset_path as training set.
    load_dataset_path: ['structure_list']   
    #load_validset_path: ['./valid.sevenn_data']


    #save_dataset_path: 'total'                   # Save the preprocessed (in load_dataset_path) dataset
    #save_by_train_valid: True                    # Save the preprocessed train.sevenn_data, valid.sevenn_data
    #save_by_label: False                         # Save the dataset by labels specified in the structure_list

